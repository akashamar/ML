{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88c872f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4eff2953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths to your rock, paper, and scissor folders\n",
    "rock_folder = 'rock/'\n",
    "paper_folder = 'paper/'\n",
    "scissor_folder = 'scissors/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24b3711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for resizing images\n",
    "image_width = 128  # Desired width\n",
    "image_height = 128  # Desired height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab057a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract HOG features from an image\n",
    "def extract_hog_features(image):\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog_features = hog.compute(image)\n",
    "    hog_features = hog_features.reshape(-1)  # Flatten the feature vector\n",
    "    return hog_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30d0a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the images\n",
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6f18f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started: rock/\n",
      "ended: rock/\n"
     ]
    }
   ],
   "source": [
    "# Load rock images\n",
    "print('started:', rock_folder)\n",
    "for filename in os.listdir(rock_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        image = cv2.imread(os.path.join(rock_folder, filename))\n",
    "        image = cv2.resize(image, (image_width, image_height))  # Resize the image\n",
    "        hog_features = extract_hog_features(image)\n",
    "        X.append(hog_features)\n",
    "        y.append(0)  # Rock class label\n",
    "print('ended:', rock_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c5d6d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started: paper/\n",
      "ended: paper/\n"
     ]
    }
   ],
   "source": [
    "# Load paper images\n",
    "print('started:', paper_folder)\n",
    "for filename in os.listdir(paper_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        image = cv2.imread(os.path.join(paper_folder, filename))\n",
    "        image = cv2.resize(image, (image_width, image_height))  # Resize the image\n",
    "        hog_features = extract_hog_features(image)\n",
    "        X.append(hog_features)\n",
    "        y.append(1)  # Paper class label\n",
    "print('ended:', paper_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19cb6558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started: scissors/\n",
      "ended: scissors/\n"
     ]
    }
   ],
   "source": [
    "# Load scissor images\n",
    "print('started:', scissor_folder)\n",
    "for filename in os.listdir(scissor_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        image = cv2.imread(os.path.join(scissor_folder, filename))\n",
    "        image = cv2.resize(image, (image_width, image_height))  # Resize the image\n",
    "        hog_features = extract_hog_features(image)\n",
    "        X.append(hog_features)\n",
    "        y.append(2)  # Scissor class label\n",
    "print('ended:', scissor_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c93df2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the feature matrix and labels to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbd22329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "901cf833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the decision tree classifier\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model on the training set\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0693828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59ebfbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8082191780821918\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28f92155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bd2a1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rock_paper_scissor_decision_tree.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'rock_paper_scissor_decision_tree.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b838b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier  # For classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17696059",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21031e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bfe9911",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "829b37cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9452054794520548\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b57b970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rock_paper_scissor_random_forest.joblib']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(model2, 'rock_paper_scissor_random_forest.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a1b00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNeighborsClassifier model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97675bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.83300132670378"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the best solution to provide k value for the KNearestNeighbors algorithm (Should be odd number)\n",
    "math.sqrt(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bad2a427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(metric=&#x27;euclidean&#x27;, n_neighbors=41, p=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(metric=&#x27;euclidean&#x27;, n_neighbors=41, p=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier(metric='euclidean', n_neighbors=41, p=3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will use k value as 41 taken from previous method, p is 3 as we have 3 class, metric euclidean to calculate distance between the neighbors\n",
    "KNearestNeighbors = KNeighborsClassifier(n_neighbors=41, p=3, metric='euclidean') \n",
    "KNearestNeighbors.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d9d1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_of_knn = KNearestNeighbors.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19aef1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9726027397260274\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_of_knn)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d98a213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knn.joblib']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(KNearestNeighbors, 'knn.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9dd1d609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using gaussian naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gaussianNB_model = GaussianNB()\n",
    "gaussianNB_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36c9dd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_of_gnb = gaussianNB_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e18787af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.906392694063927\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model's accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_of_gnb)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "235e0239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gnb.joblib']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "joblib.dump(gaussianNB_model, 'gnb.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4b4ed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using svm model\n",
    "from sklearn.svm import SVC\n",
    "# ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']\n",
    "svc_model = SVC(kernel='poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7cb1647e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "203a4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dcf0076f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svc.joblib']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(svc_model, 'svc.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e1c1d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akash/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# k-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 3  # Number of clusters (rock, paper, scissor)\n",
    "\n",
    "# Perform K-means clustering\n",
    "kmeans_model = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "kmeans_model.fit(X)\n",
    "\n",
    "# Assign labels to the cluster centers\n",
    "cluster_labels = kmeans_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9411d51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map cluster labels to class labels\n",
    "class_labels = np.zeros(num_clusters, dtype=int)\n",
    "for cluster in range(num_clusters):\n",
    "    mask = (cluster_labels == cluster)\n",
    "    class_labels[cluster] = np.bincount(y[mask]).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "683a5f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class labels for the training data\n",
    "y_pred = [class_labels[label] for label in cluster_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f800db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.6279707495429616\n"
     ]
    }
   ],
   "source": [
    "# Calculate the training accuracy\n",
    "train_accuracy = accuracy_score(y, y_pred)\n",
    "print('Training Accuracy:', train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6cd51fab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kmeans.joblib']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(kmeans_model, 'kmeans.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31d84da",
   "metadata": {},
   "source": [
    "# Train a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b41bd7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34020,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ab4dbfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:47:30.210414: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-02 23:47:31.567490: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:838] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-02 23:47:31.603751: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:838] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-02 23:47:31.603912: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:838] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-02 23:47:31.605398: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:838] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-02 23:47:31.605668: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:838] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-02 23:47:31.605760: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:838] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-02 23:47:31.605964: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:838] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-02 23:47:31.606067: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:838] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-02 23:47:31.606167: I tensorflow/compiler/xla/stream_executor/rocm/rocm_gpu_executor.cc:838] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-01-02 23:47:31.606209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7630 MB memory:  -> device: 0, name: AMD Radeon RX 6600, pci bus id: 0000:2d:00.0\n",
      "2024-01-02 23:47:31.902298: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.053813: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.054804: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.060040: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.061008: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.140183: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.143417: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.153376: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.158298: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.164550: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.168319: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.169214: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.172514: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.180914: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.181830: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.196361: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.197714: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.199239: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.200058: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.208654: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.209940: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.211526: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.212382: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.220456: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.221623: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:47:32.614955: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.618276: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.628227: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.629603: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.641331: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.643139: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.759880: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.763970: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.773379: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.774784: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.775891: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.777037: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.778087: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.778863: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.787720: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.792857: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.793643: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.801620: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.808318: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:32.813344: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:33.289191: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:34.088134: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a4797bad20 initialized for platform ROCM (this does not guarantee that XLA will be used). Devices:\n",
      "2024-01-02 23:47:34.088167: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): AMD Radeon RX 6600, AMDGPU ISA version: gfx1030\n",
      "2024-01-02 23:47:34.130881: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-01-02 23:47:34.781959: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/55 [===========================>..] - ETA: 0s - loss: 1.1992 - accuracy: 0.6689"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:47:36.894176: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:36.894861: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:36.895350: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:36.990405: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:36.991561: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:37.001099: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:37.002532: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:37.044680: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:37.045817: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:37.057997: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:37.059314: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:37.071525: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:37.078239: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:37.083908: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:37.085801: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.86073, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:47:37.182654: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "/home/akash/.local/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "2024-01-02 23:47:37.314548: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:37.402290: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:37.487709: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:37.489418: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:37.493034: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:37.495337: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 5s 25ms/step - loss: 1.1641 - accuracy: 0.6766 - val_loss: 0.4164 - val_accuracy: 0.8607\n",
      "Epoch 2/5\n",
      "51/55 [==========================>...] - ETA: 0s - loss: 0.2970 - accuracy: 0.9112\n",
      "Epoch 2: val_accuracy improved from 0.86073 to 0.89498, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:47:38.151737: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:38.155042: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:38.157640: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 1s 20ms/step - loss: 0.2940 - accuracy: 0.9114 - val_loss: 0.3003 - val_accuracy: 0.8950\n",
      "Epoch 3/5\n",
      "55/55 [==============================] - ETA: 0s - loss: 0.2195 - accuracy: 0.9274\n",
      "Epoch 3: val_accuracy improved from 0.89498 to 0.92009, saving model to best_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:47:39.782544: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:39.788222: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:39.792657: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 2s 30ms/step - loss: 0.2195 - accuracy: 0.9274 - val_loss: 0.2255 - val_accuracy: 0.9201\n",
      "Epoch 4/5\n",
      "53/55 [===========================>..] - ETA: 0s - loss: 0.1483 - accuracy: 0.9534"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:47:41.368290: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:41.373923: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:41.378429: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_accuracy improved from 0.92009 to 0.94749, saving model to best_model.h5\n",
      "55/55 [==============================] - 2s 33ms/step - loss: 0.1490 - accuracy: 0.9526 - val_loss: 0.1770 - val_accuracy: 0.9475\n",
      "Epoch 5/5\n",
      "54/55 [============================>.] - ETA: 0s - loss: 0.1816 - accuracy: 0.9398\n",
      "Epoch 5: val_accuracy did not improve from 0.94749\n",
      "55/55 [==============================] - 1s 16ms/step - loss: 0.1828 - accuracy: 0.9394 - val_loss: 0.2534 - val_accuracy: 0.9087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:47:43.106167: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:43.112876: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:43.117608: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(34020,)),  # Input layer with 34020 units\n",
    "    keras.layers.Dense(500, activation='tanh'),   # hidden layer\n",
    "    keras.layers.Dense(250, activation='tanh'),   # hidden layer\n",
    "    keras.layers.Dense(3, activation='sigmoid') # Last layer with 3 neuron (output layer)\n",
    "])\n",
    "\n",
    "# Define the ModelCheckpoint callback\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best_model.h5',   # Path to save the best model\n",
    "    monitor='val_accuracy',      # Metric to monitor (validation accuracy in this case)\n",
    "    save_best_only=True,         # Save only the best model\n",
    "    save_weights_only=False,     # Save the entire model, including architecture and optimizer state\n",
    "    mode='max',                  # Mode of the monitored metric ('max' for accuracy, 'min' for loss, etc.)\n",
    "    verbose=1                    # Verbosity mode (1: print notification when a new best model is saved)\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model with EarlyStopping callback\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_data=(X_test, y_test),  # Pass the validation data directly here\n",
    "    callbacks=[checkpoint_callback]  # Add the ModelCheckpoint callback to the callbacks list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ff46962f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:47:43.512093: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:43.515881: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:43.519687: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:43.521125: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:43.522416: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:43.523754: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:43.638528: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:43.649874: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:43.697176: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:43.706245: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:43.711916: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 8ms/step - loss: 0.1770 - accuracy: 0.9475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:47:43.719951: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.17700980603694916, 0.9474886059761047]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('best_model.h5')\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef3f8dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/14 [=>............................] - ETA: 1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:47:44.018770: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.029600: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.030946: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.070411: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.071575: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.078285: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.079524: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.080589: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.081720: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.086197: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.090700: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.096438: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.100178: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.176495: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:47:44.352760: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.354385: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.364581: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.367415: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.368749: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.392705: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.419333: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_predicted = model.predict(X_test)\n",
    "y_predicted_labels = [np.argmax(i) for i in y_predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d2471d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-02 23:47:44.484756: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.499928: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.515427: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.537923: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.544770: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.549889: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.550777: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.566314: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.567639: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-02 23:47:44.570065: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
       "array([[133,   1,   4],\n",
       "       [  1, 130,  14],\n",
       "       [  1,   2, 152]], dtype=int32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "cm = tf.math.confusion_matrix(labels=y_test,predictions=y_predicted_labels)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d1501f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(95.72222222222221, 0.5, 'Truth')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c79d0916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neural_network.joblib']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'neural_network.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "96249bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the images\n",
    "X = []\n",
    "y = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rock images\n",
    "for filename in os.listdir(rock_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        image = cv2.imread(os.path.join(rock_folder, filename))\n",
    "        image = cv2.resize(image, (image_width, image_height))  # Resize the image\n",
    "        X.append(image)\n",
    "        y.append(0)  # Rock class label\n",
    "\n",
    "for filename in os.listdir(paper_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        image = cv2.imread(os.path.join(paper_folder, filename))\n",
    "        image = cv2.resize(image, (image_width, image_height))  # Resize the image\n",
    "        X.append(image)\n",
    "        y.append(1)  # Paper class label\n",
    "\n",
    "for filename in os.listdir(scissor_folder):\n",
    "    if filename.endswith('.png'):\n",
    "        image = cv2.imread(os.path.join(scissor_folder, filename))\n",
    "        image = cv2.resize(image, (image_width, image_height))  # Resize the image\n",
    "        X.append(image)\n",
    "        y.append(2)  # Scissor class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2188, 128, 128, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 56, 140,  39], dtype=uint8)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the feature matrix and labels to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "X_train[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.21960784, 0.54901961, 0.15294118])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled = X_train / 255\n",
    "X_test_scaled = X_test / 255\n",
    "X_train_scaled[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 16:22:55.329628: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:22:55.330227: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:22:55.330941: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:22:55.331361: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    }
   ],
   "source": [
    "cnn = models.Sequential([\n",
    "    layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 16:23:00.104343: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:23:00.111333: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:23:00.265598: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:23:00.271473: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:23:00.274529: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:23:00.276960: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:23:00.560287: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 2s 29ms/step - loss: 0.9400 - accuracy: 0.6177\n",
      "Epoch 2/10\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 0.3532 - accuracy: 0.8709\n",
      "Epoch 3/10\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 0.1610 - accuracy: 0.9503\n",
      "Epoch 4/10\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 0.0813 - accuracy: 0.9760\n",
      "Epoch 5/10\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 0.0692 - accuracy: 0.9777\n",
      "Epoch 6/10\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 0.0233 - accuracy: 0.9966\n",
      "Epoch 7/10\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 0.0085 - accuracy: 0.9994\n",
      "Epoch 8/10\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "55/55 [==============================] - 2s 29ms/step - loss: 0.0011 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff6c5c2a3e0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(X_train_scaled, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/14 [=>............................] - ETA: 1s - loss: 0.1376 - accuracy: 0.9375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 16:23:23.132927: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:23:23.142292: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:23:23.190710: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:23:23.196014: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:23:23.199089: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:23:23.201523: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:23:23.254306: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 12ms/step - loss: 0.1892 - accuracy: 0.9612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1892329305410385, 0.9611872434616089]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.evaluate(X_test_scaled,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, 3)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captured_image = cv2.imread(os.path.join('.', 'paper2.png'))\n",
    "captured_image = captured_image / 255\n",
    "captured_image = cv2.resize(captured_image, (image_width, image_height))\n",
    "captured_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-06 16:25:42.100285: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:25:42.126139: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:25:42.143629: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:25:42.158653: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:25:42.161782: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n",
      "2024-01-06 16:25:42.163861: I tensorflow/core/common_runtime/gpu_fusion_pass.cc:508] ROCm Fusion is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = cnn.predict(np.array([captured_image]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.6830942e-07, 8.7844723e-01, 1.2155228e-01], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_classes = [np.argmax(element) for element in y_pred]\n",
    "y_classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
